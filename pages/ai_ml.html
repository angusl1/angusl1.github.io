<!DOCTYPE HTML>
<!--
	Phantom by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
<head>
	<title>Self Driving Simulated Robot</title>
	<meta charset="utf-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
	<link rel="stylesheet" href="../assets/css/main.css" />
	<noscript><link rel="stylesheet" href="../assets/css/noscript.css" /></noscript>
</head>
<body class="is-preload">
	<!-- Wrapper -->
	<div id="wrapper">

		<!-- Header -->
		<header id="header">
			<div class="inner">

				<!-- Logo -->
				<a href="../index.html" class="logo">
					<span class="title">Angus Li</span>
				</a>

				<!-- Nav -->
				<nav>
					<ul>
						<li><a href="#menu">Menu</a></li>
					</ul>
				</nav>

			</div>
		</header>

		<!-- Menu -->
		<nav id="menu">
			<h2>Menu</h2>
			<ul>
				<li><a href="../index.html">Home</a></li>
				<li><a href="aboutme.html">About Me</a></li>
				<li><a href="resume.html">Resume</a></li>
				<li><a href="contact.html">Contact</a></li>
			</ul>
		</nav>

		<!-- Main -->
		<div id="main">
			<div class="inner">
				<h1>ENPH353 Autonomous Detective Project</h1>
				<p style="color:red;"><b>Note: This page is in progress as I continue through the course.</b></p>
				<p>
					This page documents my journey and learning through ENPH353, a project course where we learn about computer vision, image matching algorithms, neural networks, and reinforcement learning with the goal of
					programming a simulated autonomous detective to see a simulated environment and perform various tasks to solve a mystery. Oh, it's a competition too. We have to complete as many tasks as quickly as possible
					while competing against our peers. The format of the course involves weekly labs with a code review session, then a few weeks working on the competition using the knowledge we got from the weekly labs.
				</p>
				<p>
					The course uses Linux since it allows for us to have more control over the system and run a few programs exclusive to Linux, such as ROS. We also used the terminal a lot. Since this course dives pretty in-depth into a lot of different
					technologies very quickly, and has a significant focus on artificial intelligence and machine learning, we were permitted and encouraged to use AI tools such as ChatGPT and GitHub Copilot. To make sure we
					are understanding the code that these tools spit out, we also have code review with various instructors where we review each lab's code.
				</p>
				<h2>Learning Concepts</h2>
				<p>
					The first 10 weeks of the course were based around learning about various machine learning and artificial intelligence concepts through separate labs. All of these concepts would end up being useful for our final competition. 
				<h3>Computer Vision</h3>
				<p>
					<div class="image right"><img src="../images/opencv_frame.png" alt="processed frame" /><figcaption>A frame processed by the code </figcaption></div>
					The first lab was spent reading about OpenCV, writing some skeleton code based on a given example, then fixing it to draw a circle on the path in each frame. The algorithm is relatively simple once I found out
					the functions I needed to implement it: downscale the image to speed up the processing, convert the image to grayscale and blur it, then find the edges. Then we can find the center of the two edges, which should be the center of the path and draw a circle on it.
					By processing each frame of the given video and saving it to a new video, we got this. Note that when there are no edges detected, the center defaults to the bottom left corner. I fixed it before the next lab to be
					in the last known location of the path center. My lab partner used a slightly modified approach which we used for the competition, HSV masking. By masking for the <link><a href="https://en.wikipedia.org/wiki/HSL_and_HSV" target="_blank">Hue, Saturation, and Value (brightness)</a>
					values, we could mask for certain colour ranges. In this scenario it was easy to just mask the path color, and by finding the centroid of the masked object it was pretty simple to find the centre to follow. 
				</p>
				<p>
					Another computer vision technique we did a lab on was SIFT (Scale-Invariant Feature Transform). By generating a set of feature keypoints described by vectors for a given image, objects can be identified by comparing keypoints and description vectors. 
					That was an oversimplified explanation of how SIFT works, <link><a href="https://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf" target="_blank">the creator Dr. David G. Lowe published a paper which you can read here</a>. The lab I worked on here was a simple GUI and Python script
					which the camera feed was compared with uploaded images. When a similar object was identified in the camera output, a bounding box was drawn. SIFT could have been applied to our competition project by detecting clueboards or other objects on the path, but we chose to go for another
					approach to solve these problems. 
				<h3>ROS (Robot Operating System)</h3>
				<p>
					In the second lab, we started by setting up our ROS environment. ROS is a compilation of software and tools which allows us to build and simulate robots, and for this lab we would simulate a robot
					and then control it. I spent a lot of time fiddling around in the terminal setting up my ROS environment where I managed to launch a robotless environment with only an image as a track. After, I launched the given robot and added camera
					and control plugins so we could get information from the robot camera link and control it later. Now, I had this useless robot which didn't do anything, so I had to write a python script to get it going.
					<br /><br />
					This was the fun part of the lab for me. I started by copying some example code which uses the ROS Publisher and Subscriber model and modified to my needs. This meant subscribing to the simulated robot camera feed, and publishing to
					the control module of the robot. I added the path tracking code from the first lab to process the camera input, and used the calculated path center to see how far from the center of the path we were. Then I wrote some really simple PID
					code (well I only used P) to steer the robot left or right depending on the error, and used OpenCV to show the processed frame for debugging. By setting up this environment, we will be able to follow the track in the competition.
				</p>
				<h2>Convolutional Neural Network (CNN) for Character Recognition</h2>
				<p>
					We also learned about convolutional neural networks, and implemented our own classification model which would read license plates and classify the characters. I used Google Colab, Python, and Tensorflow to create my model.
					You can see the model specifications, an example of the model predicting the character in the image, along with a confusion matrix heatmap which allows us to see what the model still gets confused about. The most interesting portion of this activity
					was seeing how the training would be affected based on the training data which we gave it. We found that training on more images for longer (more epochs) was better up until a point, at which overfitting became an issue. Overfitting was mostly resolved
					by monitoring the training and validation losses, as well as using more blurred, shifted, and rotating images. The goal was to train the neural network to understand the factors which differentiated a Q from an O, rather than just having it match pixels.
					I ended up using a modification of this architecture for our final competition CNN, where I adjusted the image input sizing and training set. 
				</p>
				<div class="box alt">
					<div class="row gtr-uniform">
						<div class="col-4">
							<span class="image fit">
								<img src="../images/353_cnn_model.png" alt="My model's specifications" />
								<figcaption> CNN Model Specifications </figcaption>
							</span>
						</div>
						<div class="col-4">
							<span class="image fit">
								<img src="../images/353_cnn_example.png" alt="Predicting '8'" />
								<figcaption>CNN Prediction</figcaption>
							</span>
						</div>
						<div class="col-4">
							<span class="image fit">
								<img src="../images/353_confusion_matrix.png" alt="Heatmap" />
								<figcaption> Confusion matrix showing prediction accuracy </figcaption>
							</span>
						</div>
					</div>
				</div>
				<h3>Machine Learning Exercises</h3>
				<p>
					The last few labs we worked on were machine learning activities where we used Q learning to train a couple of models to control robots simulated in ROS. Q-learning is a machine learning technique which uses the random actions and a reward system which you may have heard about before. 
					The model trains by taking random actions and receiving positive or negative rewards based on the outcome of the random actions. As training progresses, we hope that the model learns to prefer certain actions based on rewards given in previous states, and adjusts the Q-values appropriately. Over time, the
					training model will take fewer random actions and more actions determined by the tuned Q values. 
					<br /><br />
					The first lab was focused around taking a robot around a track (Monza in this case). To find the state, the camera frame was partitioned into 10 vertical segments and the centroid of the path was found using edge detection techniques. By assigning the centroid to a vertical segment, we 
					could provide the model with the state. The reward system was also quite simple, the robot received a better reward for keeping the centroid in the centre of the frame, and a worse reward if the centroid was further from the centre after an action. Finally, we started a new training run
					after the path went out of frame for over 3 seconds. Everytime a training run reached a high score by staying on the track longer, the Q-values of that run were copied for all future runs. In the screenshot of training below, the episode counter is quite high cause I forgot to save the
					best Q-values after each best run. Alas, we can still see the reward increasing as the model learns parts of the track, then plateauing again as it learns new portions, and so on. 
					<div class="image fit"><img src="../images/353_qlearn1.png" alt="The machine is learning!" /><Figcaption>Setup showing the track and the reward values.</Figcaption></div>
					<br /><br />
					The second lab was the <link /><a href="https://www.tensorflow.org/tutorials/reinforcement_learning/actor_critic" target="_blank">classic cartpole reinforcement learning exercise</a> with a few additions for faster training. A cartpole is simply an vertical pole pinned to a cart on a track. 
					Initially the system is at perfect equilibrium, so the pole stays upright. However, as the cart moves, the pole will tip over so the goal of the model is to keep the cartpole object on the finite track without the pole tipping past a certain angle. In our environment, 
					we gave the model two actions: movement to the left or right, along with the position, velocity, and angle of the pole from the vertical for the state. We chose to just give the model a reward for every second it stayed upright on the track, and ended the training run whenever the system
					went off the end of the track or the pole came more than 20 degrees from the vertical. I also chose to train the model in batches, taking only the top 30% of each batch to train the model off of. 
				</p>
			</div>
		</div>

		<!-- Footer -->
		<footer id="footer">
			<div class="inner">
				<section>
					<h2>Follow</h2>
					<ul class="icons">
						<li><a href="https://www.linkedin.com/in/angusl1/" class="icon brands style2 fa-linkedin" target="_blank"><span class="label">LinkedIn</span></a></li>
						<li><a href="https://github.com/angusl1/" class="icon brands style2 fa-github" target="_blank"><span class="label">GitHub</span></a></li>
						<li><a href="mailto:anguslixd@gmail.com" class="icon solid style2 fa-envelope"><span class="label">Email</span></a></li>
					</ul>
				</section>
				<ul class="copyright">
					<li>&copy; Angus Li. All rights reserved</li>
					<li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
				</ul>
			</div>
		</footer>

	</div>

	<!-- Scripts -->
	<script src="../assets/js/jquery.min.js"></script>
	<script src="../assets/js/browser.min.js"></script>
	<script src="../assets/js/breakpoints.min.js"></script>
	<script src="../assets/js/util.js"></script>
	<script src="../assets/js/main.js"></script>

</body>
</html>